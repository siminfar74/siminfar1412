<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Siminfar Samakoush Galougah</title>

    <meta name="author" content="Siminfar Samakoush Galougah">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Siminfar Samakoush Galougah
                </p>
                <p>
                    I’m a third-year PhD student at the University of Maryland, College Park, where I work under the supervision of Professor Ramani Duraiswami.
                </p>
                <p style="text-align:center">
                  <a href="mailto:simin95@umd.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/resume_samakoush.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/siminfar-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=VMx-jegAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/siminfar74/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/siminfar.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computational Audio, Differentiable Programming, Beamforming, and Machine Learning. I have a couple of papers in 5G, and wireless related areas which are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ever.github.io/">
			<span class="papertitle">EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis
</span>
        </a>
        <br>
				<a href="https://half-potato.gitlab.io/">Alexander Mai</a>, 
				<a href="https://phogzone.com/">Peter Hedman</a>,
				<a href="https://grgkopanas.github.io/">George Kopanas</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://scholar.google.com/citations?user=ozNFrecAAAAJ&hl=en">David Futschik</a>,
        <a href="https://xharlie.github.io/">Qiangeng Xu</a>,
        <a href="https://jacobsschool.ucsd.edu/faculty/profile?id=253">Falko Kuester</a>,
				<strong>Jonathan T. Barron</strong>,
        <a href="https://www.zhangyinda.com/">Yinda Zhang</a>
				<br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://half-potato.gitlab.io/posts/ever/">project page</a>
        /
        <a href="https://arxiv.org/abs/2410.01804">arXiv</a>
        <p></p>
        <p>
				Raytracing constant-density ellipsoids yields more accurate and flexible radiance fields than splatting Gaussians, and still runs in real-time.
        </p>
      </td>
    </tr>

    <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cat3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat3d_start() {
            document.getElementById('cat3d_image').style.opacity = "1";
          }

          function cat3d_stop() {
            document.getElementById('cat3d_image').style.opacity = "0";
          }
          cat3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">CAT3D: Create Anything in 3D with Multi-View Diffusion Models
</span>
        </a>
        <br>
				<a href="https://ruiqigao.github.io/">Ruiqi Gao</a>*,
        <a href="https://holynski.org/">Aleksander Holynski</a>*, 
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>, 
				<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
				<strong>Jonathan T. Barron</strong>,
        <a href="https://poolio.github.io/">Ben Poole</a>*

        <br>
        <em>NeurIPS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://cat3d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
        <p></p>
        <p>
				A single model built around diffusion and NeRF that does text-to-3D, image-to-3D, and few-view reconstruction, trains in 1 minute, and renders at 60FPS in a browser.
        </p>
      </td>
    </tr>


    <tr onmouseout="nerfcasting_stop()" onmouseover="nerfcasting_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nerfcasting_image'><video  width=100% muted autoplay loop>
          <source src="images/nerfcasting.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nerfcasting.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nerfcasting_start() {
            document.getElementById('nerfcasting_image').style.opacity = "1";
          }

          function nerfcasting_stop() {
            document.getElementById('nerfcasting_image').style.opacity = "0";
          }
          nerfcasting_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections</span>
        </a>
        <br>
				
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://phogzone.com/">Peter Hedman</a>,
				<a href="https://benattal.github.io/">Benjamin Attal</a>, <br>
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH Asia</em>, 2024
        <br>
        <a href="https://nerf-casting.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.14871">arXiv</a>
        <p></p>
        <p>
        Carefully casting reflection rays lets us synthesize photorealistic specularities in real-world scenes.
        </p>
      </td>
    </tr>


    <tr onmouseout="flash_cache_stop()" onmouseover="flash_cache_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='flash_cache_image'><video  width=100% muted autoplay loop>
          <source src="images/flash_cache.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/flash_cache.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function flash_cache_start() {
            document.getElementById('flash_cache_image').style.opacity = "1";
          }

          function flash_cache_stop() {
            document.getElementById('flash_cache_image').style.opacity = "0";
          }
          flash_cache_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://benattal.github.io/flash-cache/">
          <span class="papertitle">Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering</span>
        </a>
        <br>
				<a href="https://benattal.github.io/">Benjamin Attal</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>, <br>
				<strong>Jonathan T. Barron</strong>,
        <a href="https://www.cs.cmu.edu/~motoole2/">Matthew O'Toole</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>
        <br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://benattal.github.io/flash-cache/">project page</a>
        /
        <a href="TODO">arXiv</a>
        <p></p>
        <p>
          A more physically-accurate inverse rendering system based on radiance caching for recovering geometry, materials, and lighting from RGB images of an object or scene.
        </p>
      </td>
    </tr>



    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="images/nuvo.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nuvo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <span class="papertitle">Nuvo: Neural UV Mapping for Unruly 3D Representations</span>
        </a>
        <br>
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
        <p></p>
        <p>
        Neural fields let you recover editable UV mappings for the challenging geometries produced by NeRF-like models.
        </p>
      </td>
    </tr>


    <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bog_image'><video  width=100% muted autoplay loop>
          <source src="images/bog.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bog.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://creiser.github.io/binary_opacity_grid/">
          <span class="papertitle">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis
</span>
        </a>
        <br>
				<a href="https://creiser.github.io/">Christian Reiser</a>,
				<a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
				<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://dorverbin.github.io/">Dor Verbin</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<strong>Jonathan T. Barron</strong>,
				<a href="https://phogzone.com/">Peter Hedman</a>*,
				<a href="https://www.cvlibs.net/">Andreas Geiger</a>*		
        <br>
        <em>SIGGRAPH</em>, 2024
        <br>
        <a href="https://creiser.github.io/binary_opacity_grid/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=2TPUmGRg8bM">video</a>
        /
        <a href="https://arxiv.org/abs/2402.12377">arXiv</a>
        <p></p>
        <p>
        Applying anti-aliasing to a discrete opacity grid lets you render a hard representation into a soft image, and this enables highly-detailed mesh recovery.
        </p>
      </td>
    </tr>

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/smerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smerf.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://smerf-3d.github.io/">
          <span class="papertitle">SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</span>
        </a>
        <br>
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>,
		<a href="https://phogzone.com/">Peter Hedman*</a>,
		<a href="https://creiser.github.io/">Christian Reiser</a>,
		<a href="">Peter Zhizhin</a>,
		<a href="">Jean-François Thibert</a>,
        <a href="https://lucic.ai/">Mario Lučić</a>,
        <a href="https://szeliski.org/">Richard Szeliski</a>,
		<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH</em>, 2024 &nbsp <font color="red"><strong>(Honorable Mention)</strong></font>
        <br>
        <a href="https://smerf-3d.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
      </td>
    </tr>
	



  <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='eclipse_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/eclipse_after.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/eclipse_before.jpg' width="160">
      </div>
      <script type="text/javascript">
        function eclipse_start() {
          document.getElementById('eclipse_image').style.opacity = "1";
        }

        function eclipse_stop() {
          document.getElementById('eclipse_image').style.opacity = "0";
        }
        eclipse_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://dorverbin.github.io/eclipse">
        <span class="papertitle">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</span>
      </a>
      <br>
      <a href="https://dorverbin.github.io/">Dor Verbin</a>,
      <a href="https://bmild.github.io/">Ben Mildenhall</a>,
      <a href="https://phogzone.com/">Peter Hedman</a>, <br>
      <strong>Jonathan T. Barron</strong>,
      <a href="Todd Zickler">Todd Zickler</a>,
      <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
      <br>
      <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
      <br>
      <a href="https://dorverbin.github.io/eclipse">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
      /
      <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
      <p></p>
      <p>
      Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and materials.
      </p>
    </td>
  </tr>


  <tr onmouseout="recon_stop()" onmouseover="recon_start()" bgcolor="#ffffd0">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/recon.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/recon.png' width="160">
      </div>
      <script type="text/javascript">
        function recon_start() {
          document.getElementById('recon_image').style.opacity = "1";
        }

        function recon_stop() {
          document.getElementById('recon_image').style.opacity = "0";
        }
        recon_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://reconfusion.github.io/">
		<span class="papertitle">ReconFusion: 3D Reconstruction with Diffusion Priors</span>
      </a>
      <br>
      <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu*</a>,
	<a href="https://bmild.github.io/">Ben Mildenhall*</a>,
      <a href="https://henzler.github.io/">Philipp Henzler</a>,
      <a href="https://keunhong.com/">Keunhong Park</a>,
      <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
      <a href="https://scholar.google.com/citations?user=_pKKv2QAAAAJ&hl=en/">Daniel Watson</a>,
      <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
      <a href="https://dorverbin.github.io/">Dor Verbin</a>,
	<strong>Jonathan T. Barron</strong>,
      <a href="https://poolio.github.io/">Ben Poole</a>,
      <a href="https://holynski.org/">Aleksander Holynski*</a>
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://reconfusion.github.io/">project page</a>
      /
      <a href="https://arxiv.org/abs/2312.02981">arXiv</a>
      <p></p>
      <p>
      Using a multi-image diffusion model as a regularizer lets you recover high-quality radiance fields from just a handful of images.
      </p>
    </td>
  </tr>

  <tr onmouseout="shinobi_stop()" onmouseover="shinobi_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='shinobi_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/shinobi.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/shinobi.jpg' width="160">
      </div>
      <script type="text/javascript">
        function shinobi_start() {
          document.getElementById('shinobi_image').style.opacity = "1";
        }

        function shinobi_stop() {
          document.getElementById('shinobi_image').style.opacity = "0";
        }
        shinobi_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://shinobi.aengelhardt.com/">
        <span class="papertitle">SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-Wild</span>
      </a>
      <br>
			
			<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/">Andreas Engelhardt</a>, 
			<a href="https://amitraj93.github.io/">Amit Raj</a>, 
			<a href="https://markboss.me/">Mark Boss</a>, 
			<a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>, 
			<a href="https://abhishekkar.info/">Abhishek Kar</a>, 
			<a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
			<a href="https://deqings.github.io/">Deqing Sun</a>, 
			<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
      <strong>Jonathan T. Barron</strong>,
			<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/">Hendrik P.A. Lensch</a>, 
			<a href="https://varunjampani.github.io/">Varun Jampani</a>
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://shinobi.aengelhardt.com/">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=m_5kvtlDnl4">video</a>
      /
      <a href="https://arxiv.org/abs/2401.10171">arXiv</a>
      <p></p>
      <p>
      A class-agnostic inverse rendering solution for turning in-the-wild images of an object into a relightable 3D asset.
      </p>
    </td>
  </tr>
	

    <tr onmouseout="internerf_stop()" onmouseover="internerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='internerf_image'>
					  <img src='images/internerf_after.jpg' width=100%>
					</div>
          <img src='images/internerf_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function internerf_start() {
            document.getElementById('internerf_image').style.opacity = "1";
          }

          function internerf_stop() {
            document.getElementById('internerf_image').style.opacity = "0";
          }
          internerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.11737">
          <span class="papertitle">InterNeRF: Scaling Radiance Fields via Parameter Interpolation</span>
        </a>
        <br>
		<a href="https://clintonjwang.github.io/">Clinton Wang</a>,
		<a href="https://phogzone.com/">Peter Hedman</a>,
		<a href="https://people.csail.mit.edu/polina/">Polina Golland</a>,
		<strong>Jonathan T. Barron</strong>,
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
        <br>
        <em>CVPR Neural Rendering Intelligence</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2406.11737">arXiv</a>
        <p></p>
        <p>
        Parameter interpolation enables high-quality large-scale scene reconstruction and out-of-core training and rendering.
        </p>
      </td>
    </tr>


<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='difsurvey_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/difsurvey_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/difsurvey_image.jpg' width="160">
    </div>
    <script type="text/javascript">
      function difsurvey_start() {
        document.getElementById('difsurvey_image').style.opacity = "1";
      }

      function difsurvey_stop() {
        document.getElementById('difsurvey_image').style.opacity = "0";
      }
      difsurvey_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2310.07204">
      <span class="papertitle">State of the Art on Diffusion Models for Visual Computing
</span>
    </a>
    <br>
	<a href="https://ryanpo.com/">Ryan Po</a>,
	<a href="https://yifita.netlify.app/">Wang Yifan</a>,
	<a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>,
	<a href="https://kfiraberman.github.io/">Kfir Aberman</a>,
	<strong>Jonathan T. Barron</strong>,
	<a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
	<a href="https://ericryanchan.github.io/">Eric Ryan Chan</a>,
	<a href="https://www.weizmann.ac.il/math/dekel/home">Tali Dekel</a>,
	<a href="https://holynski.org/">Aleksander Holynski</a>,
	<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
	<a href="https://tml.stanford.edu/">C. Karen Liu</a>,
	<a href="https://lingjie0206.github.io/">Lingjie Liu</a>,
	<a href="https://bmild.github.io/">Ben Mildenhall</a>,
    <a href="https://www.niessnerlab.org/">Matthias Nießner</a>,
	<a href="https://ommer-lab.com/people/ommer/">Björn Ommer</a>,
	<a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>,
	<a href="https://peterwonka.net/">Peter Wonka</a>,
    <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
    <br>
	<em>Eurographics State-of-the-Art Report<em>, 2024
    <br>
    <p></p>
    <p>
    A survey of recent progress in diffusion models for images, videos, and 3D.
    </p>
  </td>
</tr>          

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/camp.png' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://camp-nerf.github.io/">
          <span class="papertitle">CamP: Camera Preconditioning for Neural Radiance Fields</span>
        </a>
        <br>
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>
        <br>
        <em>SIGGRAPH Asia</em>, 2023
        <br>
        <a href="https://camp-nerf.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2308.10902">arXiv</a>
        <p></p>
        <p>
        Preconditioning based on camera parameterization helps NeRF and camera extrinsics/intrinsics optimize better together.
        </p>
      </td>
    </tr>

    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</span>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
          <br>
          <a href="http://jonbarron.info/zipnerf">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
          /
          <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
          <p></p>
          <p>
          Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x.
          </p>
        </td>
      </tr>
      
      
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dreambooth3d.github.io/">
            <span class="papertitle">DreamBooth3D: Subject-Driven Text-to-3D Generation</span>
          </a>
          <br>
          
  <a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
          <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://dreambooth3d.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2303.13508">arXiv</a>
          <p></p>
          <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
        </td>
      </tr>

      

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://bakedsdf.github.io/">
            <span class="papertitle">BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</span>
          </a>
          <br>
          <a href="https://lioryariv.github.io/">Lior Yariv*</a>,
          <a href="https://phogzone.com/">Peter Hedman*</a>,
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,  <br>
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://bakedsdf.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a>
          /
          <a href="https://arxiv.org/abs/2302.14859">arXiv</a>
          <p></p>
          <p>
          We use SDFs to bake a NeRF-like model into a high quality mesh and do real-time view synthesis.
          </p>
        </td>
      </tr>


      <tr onmouseout="merf_stop()" onmouseover="merf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/merf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/merf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function merf_start() {
              document.getElementById('merf_image').style.opacity = "1";
            }

            function merf_stop() {
              document.getElementById('merf_image').style.opacity = "0";
            }
            merf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://merf42.github.io/">
            <span class="papertitle">MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes</span>
          </a>
          <br>
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://merf42.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=3EACM2JAcxc">video</a>
          /
          <a href="https://arxiv.org/abs/2302.12249">arXiv</a>
          <p></p>
          <p>
          We use volumetric rendering with a sparse 3D feature grid and 2D feature planes to do real-time view synthesis.
          </p>
        </td>
      </tr>



      <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='alignerf_image'>
              <img src='images/alignerf_after.jpg' width="160"></div>
            <img src='images/alignerf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function alignerf_start() {
              document.getElementById('alignerf_image').style.opacity = "1";
            }

            function alignerf_stop() {
              document.getElementById('alignerf_image').style.opacity = "0";
            }
            alignerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://yifanjiang19.github.io/alignerf">
            <span class="papertitle">AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training</span>
          </a>
          <br>
          <a href="https://yifanjiang.net/">Yifan Jiang</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>, 
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://ir1d.github.io/">Dejia Xu</a>, <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
          <a href="https://tianfan.info/">Tianfan Xue</a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="https://yifanjiang19.github.io/alignerf">project page</a>
          /
          <a href="https://arxiv.org/abs/2211.09682">arXiv</a>
          <p></p>
          <p>
          Accounting for misalignment due to scene motion or calibration errors improves NeRF reconstruction quality.
          </p>
        </td>
      </tr>
      
  <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/dreamfusion.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/dreamfusion.jpg' width="160">
      </div>
      <script type="text/javascript">
        function dreamfusion_start() {
          document.getElementById('dreamfusion_image').style.opacity = "1";
        }

        function dreamfusion_stop() {
          document.getElementById('dreamfusion_image').style.opacity = "0";
        }
        dreamfusion_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://dreamfusion3d.github.io/">
        <span class="papertitle">DreamFusion: Text-to-3D using 2D Diffusion</span>
      </a>
      <br>
      <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
      <a href="https://www.ajayj.com/">Ajay Jain</a>,
      <strong>Jonathan T. Barron</strong>,
      <a href="https://bmild.github.io/">Ben Mildenhall</a>
      <br>
      <em>ICLR</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Outstanding Paper Award)</strong></font>
      <br>
      <a href="https://dreamfusion3d.github.io/">project page</a>
      /
      <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
      /
      <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
      <p></p>
      <p>
      We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.
      </p>
    </td>
  </tr>

  <tr onmouseout="guandao_stop()" onmouseover="guandao_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='guandao_image'>
        <img src='images/guandao_after.png' width="160"></div>
      <img src='images/guandao_before.png' width="160">
    </div>
    <script type="text/javascript">
      function guandao_start() {
        document.getElementById('guandao_image').style.opacity = "1";
      }

      function guandao_stop() {
        document.getElementById('guandao_image').style.opacity = "0";
      }
      guandao_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2304.14473">
      <span class="papertitle">Learning a Diffusion Prior for NeRFs</span>
    </a>
    <br>
    <a href="https://www.guandaoyang.com/">Guandao Yang</a>, 
    <a href="https://abhijitkundu.info/">Abhijit Kundu</a>, 
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a>, 
    <strong>Jonathan T. Barron</strong>, 
    <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
    <br>
    <em>ICLR Workshop</em>, 2023
    <p></p>
    <p>
      Training a diffusion model on grid-based NeRFs lets you (conditionally) sample NeRFs.
    </p>
  </td>
  </tr>

            <tr onmouseout="mira_stop()" onmouseover="mira_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mira_image'>
                    <img src='images/mira_after.jpg' width="160"></div>
                  <img src='images/mira_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function mira_start() {
                    document.getElementById('mira_image').style.opacity = "1";
                  }

                  function mira_stop() {
                    document.getElementById('mira_image').style.opacity = "0";
                  }
                  mira_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=AmPeAFzU3a4">
                  <span class="papertitle">MIRA: Mental Imagery for Robotic Affordances</span>
                </a>
                <br>
                <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                <a href="https://andyzeng.github.io/">Andy Zeng</a>, <strong>Jonathan T. Barron</strong>, 
                <a href="https://yilundu.github.io/">Yilun Du</a>,
                <a href="https://people.csail.mit.edu/weichium/">Wei-Chiu Ma</a>,
                <a href="https://anthonysimeonov.github.io/">Anthony Simeonov</a>,
                <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
                <br>
                <em>CoRL</em>, 2022
                <p></p>
                <p>
                  NeRF lets us synthesize novel orthographic views that work well with pixel-wise algorithms for robotic manipulation.
                </p>
              </td>
            </tr>		
            
            <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='samurai_image'>
                    <img src='images/samurai_after.jpg' width="160"></div>
                  <img src='images/samurai_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function samurai_start() {
                    document.getElementById('samurai_image').style.opacity = "1";
                  }

                  function samurai_stop() {
                    document.getElementById('samurai_image').style.opacity = "0";
                  }
                  samurai_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me/publication/2022-samurai/">
                  <span class="papertitle">SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image Collections</span>
                </a>
                <br>
                <a href="https://markboss.me">Mark Boss</a>, 
                <a href="">Andreas Engelhardt</a>, 
                <a href="https://abhishekkar.info/">Abhishek Kar</a>, 
                <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
                <a href="https://deqings.github.io/">Deqing Sun</a>, 
                <strong>Jonathan T. Barron</strong>,
                <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>,
                <a href="https://varunjampani.github.io">Varun Jampani</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                <a href="https://markboss.me/publication/2022-samurai/">project page</a> /
                <a href="https://www.youtube.com/watch?v=LlYuGDjXp-8">video</a> /
                <a href="https://arxiv.org/abs/2205.15768">arXiv</a>
                <p></p>
                <p>
  A joint optimization framework for estimating shape, BRDF, camera pose, and illumination from in-the-wild image collections.
                </p>
              </td>
            </tr>		

            <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
            <div class="two" id='pnf_image'>
              <img src='images/pnf_before.jpg' width="160"></div>
            <img src='images/pnf_after.jpg' width="160">
            </div>
            <script type="text/javascript">
            function pnf_start() {
              document.getElementById('pnf_image').style.opacity = "1";
            }

            function pnf_stop() {
              document.getElementById('pnf_image').style.opacity = "0";
            }
            pnf_stop()
            </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="TODO">
            <span class="papertitle">Polynomial Neural Fields for Subband Decomposition</span>
            </a> <br>
            <a href="https://www.guandaoyang.com/">Guandao Yang*</a>,
            <a href="https://sagiebenaim.github.io/">Sagie Benaim*</a>,
            <a href="https://varunjampani.github.io/">Varun Jampani</a>,
            <a href="https://www.kylegenova.com/">Kyle Genova</a>,
            <strong>Jonathan T. Barron</strong>,
            <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
            <a href="http://home.bharathh.info/">Bharath Hariharan</a>,
            <a href="https://sergebelongie.github.io/">Serge Belongie</a>
            <br>
            <em>NeurIPS</em>, 2022
            <p>
            Representing neural fields as a composition of manipulable and interpretable components lets you do things like reason about frequencies and scale.
            </p>
            </td>
            </tr> 


            <tr onmouseout="malle_stop()" onmouseover="malle_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/MalleConv_after.jpg' width="160"></div>
                  <img src='images/MalleConv_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function malle_start() {
                    document.getElementById('malle_image').style.opacity = "1";
                  }

                  function malle_stop() {
                    document.getElementById('malle_image').style.opacity = "0";
                  }
                  malle_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://yifanjiang.net/MalleConv.html">
                  <span class="papertitle">Fast and High-Quality Image Denoising via Malleable Convolutions</span>
                </a>
                <br>
                <a href="https://yifanjiang.net/">Yifan Jiang</a>,
                <a href="https://bartwronski.com/">Bartlomiej Wronski</a>, 
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>
                <br>
                <em>ECCV</em>, 2022
                <br>
                <a href="https://yifanjiang.net/MalleConv.html">project page</a>
                /
                <a href="https://arxiv.org/abs/2201.00392">arXiv</a>
                <p></p>
                <p>
                We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerf_supervision.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerf_supervision.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('nerfsuper_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('nerfsuper_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://yenchenlin.me/nerf-supervision/">
                  <span class="papertitle">NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</span>
                </a>
                <br>
                <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                <strong>Jonathan T. Barron</strong>,  <br>
                <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
                <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
                <br>
                <em>ICRA</em>, 2022  
                <br>
                <a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
                <a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
                <a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
                <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
                <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
                <p></p>
                <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
              </td>
            </tr>

            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/refnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/refnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function refnerf_start() {
                    document.getElementById('refnerf_image').style.opacity = "1";
                  }

                  function refnerf_stop() {
                    document.getElementById('refnerf_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dorverbin.github.io/refnerf/index.html">
                    <span class="papertitle">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</span>
                  </a>
                  <br>
                  <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                  <a href="Todd Zickler">Todd Zickler</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
            <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
                  <br>
                  <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
            /
                  <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
            /
                  <a href="https://youtu.be/qrdRH9irAlk">video</a>
                  <p></p>
                  <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>
                </td>
              </tr>
              
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/mip360_sat.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/mip360_sat.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function mip360_start() {
                    document.getElementById('mip360_image').style.opacity = "1";
                  }

                  function mip360_stop() {
                    document.getElementById('mip360_image').style.opacity = "0";
                  }
                  mip360_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://jonbarron.info/mipnerf360">
                  <span class="papertitle">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="http://jonbarron.info/mipnerf360">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
                /
                <a href="https://youtu.be/zBSH-k9GbV4">video</a>
                <p></p>
                <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
              </td>
            </tr> 

            <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/rawnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/rawnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function rawnerf_start() {
                    document.getElementById('rawnerf_image').style.opacity = "1";
                  }

                  function rawnerf_stop() {
                    document.getElementById('rawnerf_image').style.opacity = "0";
                  }
                  rawnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://bmild.github.io/rawnerf/index.html">
                  <span class="papertitle">NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</span>
                </a>
                <br>
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
          /
                <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
                <p></p>
                <p>
                  Properly training NeRF on raw camera data enables HDR view synthesis and bokeh, and outperforms multi-image denoising.</p>
              </td>
            </tr> 
            
    
            <tr onmouseout="regnerf_stop()" onmouseover="regnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='regnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/regnerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/regnerf_before.jpeg' width="160">
                </div>
                <script type="text/javascript">
                  function regnerf_start() {
                    document.getElementById('regnerf_image').style.opacity = "1";
                  }

                  function regnerf_stop() {
                    document.getElementById('regnerf_image').style.opacity = "0";
                  }
                  regnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/regnerf/index.html">
                  <span class="papertitle">RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</span>
                </a>
                <br>
                <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                <a href="https://msmsajjadi.github.io/">Mehdi S. M. Sajjadi</a>, 
                <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
                <a href="http://www2.informatik.uni-freiburg.de/~radwann/">Noha Radwan</a>
                <br>
          <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://m-niemeyer.github.io/regnerf/index.html">project page</a>
          /
                <a href="https://arxiv.org/abs/2112.00724">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=QyyyvA4-Kwc">video</a>
                <p></p>
                <p>Regularizing unseen views during optimization enables view synthesis from as few as 3 input images.</p>
              </td>
            </tr> 


            <tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/blocknerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/blocknerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function blocknerf_start() {
                    document.getElementById('blocknerf_image').style.opacity = "1";
                  }

                  function blocknerf_stop() {
                    document.getElementById('blocknerf_image').style.opacity = "0";
                  }
                  blocknerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://waymo.com/research/block-nerf/">
                  <span class="papertitle">Block-NeRF: Scalable Large Scene Neural View Synthesis</span>
                </a>
                <br>
                <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                <a href="http://casser.io/">Vincent Casser</a>,
                <a href="https://sites.google.com/site/skywalkeryxc/">Xinchen Yan</a>,
                <a href="https://scholar.google.com/citations?user=5mJUkI4AAAAJ&hl=en">Sabeek Pradhan</a>, <br>
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.henrikkretzschmar.com/">Henrik Kretzschmar</a>
                <br>
          <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://waymo.com/research/block-nerf/">project page</a>
          /
                <a href="https://arxiv.org/abs/2202.05263">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=6lGMCAzBzOQ">video</a>
                <p></p>
                <p>We can do city-scale reconstruction by training multiple NeRFs with millions of images.</p>
              </td>
            </tr>
            
            <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/hnerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/hnerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function hnerf_start() {
                    document.getElementById('hnerf_image').style.opacity = "1";
                  }

                  function hnerf_stop() {
                    document.getElementById('hnerf_image').style.opacity = "0";
                  }
                  hnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://grail.cs.washington.edu/projects/humannerf/">
                  <span class="papertitle">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</span>
                </a>
                <br>
                <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
                <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
                /
                <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
                /
                <a href="https://youtu.be/GM-RoZEymmw">video</a>
                <p></p>
                <p>Combining NeRF with pose estimation lets you use a monocular video to do free-viewpoint rendering of a human.</p>
              </td>
            </tr>
            
            <tr onmouseout="urf_stop()" onmouseover="urf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/urf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/urf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function urf_start() {
                    document.getElementById('urf_image').style.opacity = "1";
                  }

                  function urf_stop() {
                    document.getElementById('urf_image').style.opacity = "0";
                  }
                  urf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://urban-radiance-fields.github.io/">
                  <span class="papertitle">Urban Radiance Fields</span>
                </a>
                <br>
                <a href="http://www.krematas.com/">Konstantinos Rematas</a>,
                <a href="https://andrewhliu.github.io/">Andrew Liu</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
                <a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a>,
                <a href="https://sites.google.com/corp/view/vittoferrari"> Vittorio Ferrari</a>
                <br>
                <em>CVPR</em>, 2022
                <br>
                <a href="https://urban-radiance-fields.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.14643">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=qGlq5DZT6uc">video</a>
                <p></p>
                <p>
                  Incorporating lidar and explicitly modeling the sky lets you reconstruct urban environments.</p>
              </td>
            </tr> 

    
    <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ddp_image'>
            <img src='images/ddp_after.jpg' width="160"></div>
          <img src='images/ddp_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function ddp_start() {
            document.getElementById('ddp_image').style.opacity = "1";
          }

          function ddp_stop() {
            document.getElementById('ddp_image').style.opacity = "0";
          }
          ddp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2112.03288">
          <span class="papertitle">Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</span>
        </a>
        <br>
        <a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>, 
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, 
        <a href="https://www.niessnerlab.org/">Matthias Nießner</a>
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2112.03288">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>
        <p></p>
        <p>
        Dense depth completion techniques applied to freely-available sparse stereo data can improve NeRF reconstructions in low-data regimes.
        </p>
      </td>
    </tr>
    
            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='clipnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/dreamfield_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/dreamfield_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ajayj.com/dreamfields">
                  <span class="papertitle">Zero-Shot Text-Guided Object Generation with Dream Fields</span>
                </a>
                <br>
                <a href="https://www.ajayj.com/">Ajay Jain</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
                <br>
          <em>CVPR</em>, 2022
                <br>
                <a href="https://ajayj.com/dreamfields">project page</a>
          /
                <a href="https://arxiv.org/abs/2112.01455">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=1Fke6w46tv4">video</a>
                <p></p>
                <p>Supervising the CLIP embeddings of NeRF renderings lets you to generate 3D objects from text prompts.</p>
              </td>
            </tr> 
      
            <tr onmouseout="survey_stop()" onmouseover="survey_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='survey_image'>
                    <img src='images/survey_after.png' width="160"></div>
                  <img src='images/survey_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function survey_start() {
                    document.getElementById('survey_image').style.opacity = "1";
                  }

                  function survey_stop() {
                    document.getElementById('survey_image').style.opacity = "0";
                  }
                  survey_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2111.05849">
                  <span class="papertitle">Advances in Neural Rendering</span>
                </a>
                <br>
                <a href="https://people.mpi-inf.mpg.de/~atewari/">Ayush Tewari</a>, 
                <a href="https://justusthies.github.io/">Justus Thies</a>, 
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, 
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, 
                <a href="https://people.mpi-inf.mpg.de/~tretschk/">Edgar Tretschk</a>,
                <a href="https://homes.cs.washington.edu/~yifan1/">Yifan Wang</a>,
                <a href="https://christophlassner.de/">Christoph Lassner</a>,
                <a href="https://vsitzmann.github.io/">Vincent Sitzmann</a>,
                <a href="http://ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                <a href="https://stephenlombardi.github.io/">Stephen Lombardi</a>,
                <a href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>,
                <a href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Christian Theobalt</a>,
                <a href="https://www.niessnerlab.org/">Matthias Niessner</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                <a href="https://zollhoefer.com/">Michael Zollhoefer</a>,
                <a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>
                <br>
				<em>State of the Art Report at EUROGRAPHICS<em>, 2022
                <br>
                <p></p>
                <p>
                A survey of recent progress in neural rendering.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="npil_stop()" onmouseover="npil_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='npil_image'>
                    <img src='images/npil_after.jpg' width="160"></div>
                  <img src='images/npil_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function npil_start() {
                    document.getElementById('npil_image').style.opacity = "1";
                  }

                  function npil_stop() {
                    document.getElementById('npil_image').style.opacity = "0";
                  }
                  npil_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me/publication/2021-neural-pil/">
                  <span class="papertitle">Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</span>
                </a>
                <br>

                <a href="https://markboss.me">Mark Boss</a>, 
                <a href="https://varunjampani.github.io">Varun Jampani</a>,
                <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>, <br>
                <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
                <br>
                <em>NeurIPS</em>, 2021
                <br>
                <a href="https://markboss.me/publication/2021-neural-pil/">project page</a> /
                <a href="https://www.youtube.com/watch?v=p5cKaNwVp4M">video</a> /
                <a href="https://arxiv.org/abs/2110.14373">arXiv</a>
                <p></p>
                <p>
                Replacing a costly illumination integral with a simple network query enables more accurate novel view-synthesis and relighting compared to NeRD.
                </p>
              </td>
            </tr>
            
            
            <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/hypernerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/hypernerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function hypernerf_start() {
                    document.getElementById('hypernerf_image').style.opacity = "1";
                  }

                  function hypernerf_stop() {
                    document.getElementById('hypernerf_image').style.opacity = "0";
                  }
                  hypernerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://hypernerf.github.io/">
                  <span class="papertitle">HyperNeRF: A Higher-Dimensional Representation
for Topologically Varying Neural Radiance Fields</span>
                </a>
                <br>
                <a href="https://keunhong.com">Keunhong Park</a>,
                <a href="https://utkarshsinha.com">Utkarsh Sinha</a>, 
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,
                <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
                <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a>, 
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2021 
                <br>
                <a href="https://hypernerf.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2106.13228">arXiv</a>
                <p></p>
                <p>Applying ideas from level set methods to NeRF lets you represent scenes that deform and change shape.</p>
              </td>
            </tr> 

            <tr onmouseout="nerfactor_stop()" onmouseover="nerfactor_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfactor_image'>
                    <img src='images/nerfactor_after.png' width="160"></div>
                  <img src='images/nerfactor_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function nerfactor_start() {
                    document.getElementById('nerfactor_image').style.opacity = "1";
                  }

                  function nerfactor_stop() {
                    document.getElementById('nerfactor_image').style.opacity = "0";
                  }
                  nerfactor_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">
                <span class="papertitle">NeRFactor: Neural Factorization of Shape and Reflectance<br>
  Under an Unknown Illumination</span>
                </a>
                <br>
                <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://boyangdeng.com/">Boyang Deng</a>,<br>
                <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                <a href="http://billf.mit.edu/">William T. Freeman</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>SIGGRAPH Asia</em>, 2021 
                <br>
                <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">project page</a>
                /
                <a href="https://arxiv.org/abs/2106.01970">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a>
                <p></p>
                <p>By placing priors on illumination and materials, we can recover NeRF-like models of the intrinsics of a scene from a single multi-image capture.</p>
              </td>
              
            <tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualfont_image'><img src='images/dualfont_after.png'></div>
                  <img src='images/dualfont_before.png'>
                </div>
                <script type="text/javascript">
                  function dualfont_start() {
                    document.getElementById('dualfont_image').style.opacity = "1";
                  }

                  function dualfont_stop() {
                    document.getElementById('dualfont_image').style.opacity = "0";
                  }
                  dualfont_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2109.06627">
                  <span class="papertitle">Scalable Font Reconstruction with Dual Latent Manifolds</span>
                </a>
                <br>
                <a href="http://www.cs.cmu.edu/~asrivats/">Nikita Srivatsan</a>,
                <a href="http://siwu.io/">Si Wu</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~tberg/">Taylor Berg-Kirkpatrick</a>
                <br>
                <em>EMNLP</em>, 2021
                <br>
                <p></p>
                <p>VAEs can be used to disentangle a font's style from its content, and to generalize to characters that were never observed during training.</p>
              </td>
            </tr>
            
            <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/mipnerf_ipe_yellow.png' width="160">
                </div>
                <script type="text/javascript">
                  function mipnerf_start() {
                    document.getElementById('mipnerf_image').style.opacity = "1";
                  }

                  function mipnerf_stop() {
                    document.getElementById('mipnerf_image').style.opacity = "0";
                  }
                  mipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://jonbarron.info/mipnerf">
                  <span class="papertitle">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
                <br>
                <a href="http://jonbarron.info/mipnerf">project page</a>
                /
                <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
                /
                <a href="https://youtu.be/EpH175PY1A0">video</a>
                /
                <a href="https://github.com/google/mipnerf">code</a>
                <p></p>
                <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
              </td>
            </tr> 

            <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfbake_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerfbake_15.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerfbake_160.png' width="160">
                </div>
                <script type="text/javascript">
                  function nerfbake_start() {
                    document.getElementById('nerfbake_image').style.opacity = "1";
                  }

                  function nerfbake_stop() {
                    document.getElementById('nerfbake_image').style.opacity = "0";
                  }
                  nerfbake_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://nerf.live">
                <span class="papertitle">Baking Neural Radiance Fields for Real-Time View Synthesis</span>
                </a>
                <br>
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.pauldebevec.com/">Paul Debevec</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="http://nerf.live">project page</a>
                /
                <a href="https://arxiv.org/abs/2103.14645">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a>
                /
                <a href="https://nerf.live/#demos">demo</a>
                <p></p>
                <p>Baking a trained NeRF into a sparse voxel grid of colors and features lets you render it in real-time in your browser.</p>
              </td>



            <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfie_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerfie_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerfie_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerfie_start() {
                    document.getElementById('nerfie_image').style.opacity = "1";
                  }
                  function nerfie_stop() {
                    document.getElementById('nerfie_image').style.opacity = "0";
                  }
                  nerfie_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://nerfies.github.io/">
                  <span class="papertitle">Nerfies: Deformable Neural Radiance Fields</span>
                </a>
                <br>
                
                <a href="https://keunhong.com">Keunhong Park</a>,
                <a href="https://utkarshsinha.com">Utkarsh Sinha</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,
                <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>,
                <a href="http://www.ricardomartinbrualla.com">Ricardo-Martin Brualla</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://nerfies.github.io/">project page</a> /
                <a href="https://arxiv.org/abs/2011.12948">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA">video</a>
                <p></p>
                <p>Building deformation fields into NeRF lets you capture non-rigid subjects, like people.
                </p>
              </td>
            </tr> 


            <tr onmouseout="c5_stop()" onmouseover="c5_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='c5_image'>
                    <img src='images/c5_after.jpg' width="160"></div>
                  <img src='images/c5_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function c5_start() {
                    document.getElementById('c5_image').style.opacity = "1";
                  }

                  function c5_stop() {
                    document.getElementById('c5_image').style.opacity = "0";
                  }
                  c5_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2011.11890">
                  <span class="papertitle">Cross-Camera Convolutional Color Constancy</span>
                </a>
                <br>
                <a href="https://sites.google.com/corp/view/mafifi">Mahmoud Afifi</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <a href="https://www.linkedin.com/in/fbleibel/">Francois Bleibel</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <p></p>
                <p>
                  With some extra (unlabeled) test-set images, you can build a hypernetwork that calibrates itself at test time to previously-unseen cameras.
                </p>
              </td>
            </tr> 


            <tr onmouseout="dualdefocus_stop()" onmouseover="dualdefocus_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualdefocus_image'>
                    <img src='images/dualdefocus_after.jpg' width="160"></div>
                  <img src='images/dualdefocus_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function dualdefocus_start() {
                    document.getElementById('dualdefocus_image').style.opacity = "1";
                  }

                  function dualdefocus_stop() {
                    document.getElementById('dualdefocus_image').style.opacity = "0";
                  }
                  dualdefocus_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                  <span class="papertitle">Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</span>
                </a>
                <br>
                <a href="https://shumianxin.github.io/">Shumian Xin</a>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">project page</a> /
                <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">code</a>
                <br>
                <p></p>
                <p>
                  Multiplane images can be used to simultaneously deblur dual-pixel images, despite variable defocus due to depth variation in the scene.
                </p>
              </td>
            </tr> 


            <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerd_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerd_160.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerd_160.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerd_start() {
                    document.getElementById('nerd_image').style.opacity = "1";
                  }

                  function nerd_stop() {
                    document.getElementById('nerd_image').style.opacity = "0";
                  }
                  nerd_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me/publication/2021-nerd/">
                  <span class="papertitle">NeRD: Neural Reflectance Decomposition from Image Collections</span>
                </a>
                <br>

                <a href="https://markboss.me">Mark Boss</a>, 
                <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>,
                <a href="https://varunjampani.github.io">Varun Jampani</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
                <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
                <br>
                <em>ICCV</em>, 2021
                <br>
                <a href="https://markboss.me/publication/2021-nerd/">project page</a> /
                <a href="https://www.youtube.com/watch?v=JL-qMTXw9VU">video</a> /
                <a href="https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition">code</a> /
                <a href="https://arxiv.org/abs/2012.03918">arXiv</a>
                <p></p>
                <p>
                A NeRF-like model that can decompose (and mesh) objects with non-Lambertian reflectances, complex geometry, and unknown illumination.
                </p>
              </td>
            </tr>

            <tr onmouseout="flare_stop()" onmouseover="flare_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='flare_image'>
                    <img src='images/flare_after.jpg' width="160"></div>
                  <img src='images/flare_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function flare_start() {
                    document.getElementById('flare_image').style.opacity = "1";
                  }

                  function flare_stop() {
                    document.getElementById('flare_image').style.opacity = "0";
                  }
                  flare_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2011.12485">
                  <span class="papertitle">How to Train Neural Networks for Flare Removal</span>
                </a>
                <br>
                <a href="http://yicheng.rice.edu/">Yicheng Wu</a>,
                <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ">Qiurui He</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>, <br>
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>ICCV</em>, 2021
                <br>
                <a href="https://yichengwu.github.io/flare-removal/">project page</a>  / 
                <a href="https://arxiv.org/abs/2011.12485">arXiv</a> 
                <p></p>
                <p>
                  Simulating the optics of a camera's lens lets you train a model that removes lens flare from a single image.
                </p>
              </td>
            </tr> 


            <tr onmouseout="inerf_stop()" onmouseover="inerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='inerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/inerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/inerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function inerf_start() {
                    document.getElementById('inerf_image').style.opacity = "1";
                  }
                  function inerf_stop() {
                    document.getElementById('inerf_image').style.opacity = "0";
                  }
                  inerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://yenchenlin.me/inerf/">
                  <span class="papertitle">iNeRF: Inverting Neural Radiance Fields for Pose Estimation</span>
                </a>
                <br>
                <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                <strong>Jonathan T. Barron</strong>,  <br>
                <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>
                <br>
                <em>IROS</em>, 2021  
                <br>
                <a href="http://yenchenlin.me/inerf/">project page</a> /
                <a href="https://arxiv.org/abs/2012.05877">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=eQuCZaQN0tI">video</a>
                <p></p>
                <p>Given an image of an object and a NeRF of that object, you can estimate that object's pose.
                </p>
              </td>
            </tr> 

            <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ibrnet_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/ibrnet_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/ibrnet_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function ibrnet_start() {
                    document.getElementById('ibrnet_image').style.opacity = "1";
                  }

                  function ibrnet_stop() {
                    document.getElementById('ibrnet_image').style.opacity = "0";
                  }
                  ibrnet_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ibrnet.github.io/">
                  <span class="papertitle">IBRNet: Learning Multi-View Image-Based Rendering</span>
                </a>
                <br>
                <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
                <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/">Zhicheng Wang</a>,
                <a href="https://www.kylegenova.com/">Kyle Genova</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en">Howard Zhou</a>, <br>
                <strong>Jonathan T. Barron</strong>, 
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
                <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>
                <br>
                <em>CVPR</em>, 2021
                <br>
                <a href="https://ibrnet.github.io/">project page</a> /
                <a href="https://github.com/googleinterns/IBRNet">code</a> / 
                <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
                <p></p>
                <p>By learning how to pay attention to input images at render time, 
                    we can amortize inference for view synthesis and reduce error rates by 15%.</p>
              </td>
            </tr>

            <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerv_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/hotdog.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/hotdog.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerv_start() {
                    document.getElementById('nerv_image').style.opacity = "1";
                  }

                  function nerv_stop() {
                    document.getElementById('nerv_image').style.opacity = "0";
                  }
                  nerv_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pratulsrinivasan.github.io/nerv/">
                  <span class="papertitle">NeRV: Neural Reflection and Visibility Fields for Relighting and View Synthesis</span>
                </a>
                <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://boyangdeng.com/">Boyang Deng</a>,
                <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>, <br>
                <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>CVPR</em>, 2021
                <br>
                <a href="https://pratulsrinivasan.github.io/nerv/">project page</a> /
                <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
                <a href="https://arxiv.org/abs/2012.03927">arXiv</a>
                <p></p>
                <p>Using neural approximations of expensive visibility integrals lets you recover relightable NeRF-like models.</p>
              </td>
            </tr>


            <tr onmouseout="winr_stop()" onmouseover="winr_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/notre_160.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/notre.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function winr_start() {
                    document.getElementById('winr_image').style.opacity = "1";
                  }
                  function winr_stop() {
                    document.getElementById('winr_image').style.opacity = "0";
                  }
                  winr_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www.matthewtancik.com/learnit">
                  <span class="papertitle">Learned Initializations for Optimizing Coordinate-Based Neural Representations</span>
                </a>
                <br>
                <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                <a href="https://www.linkedin.com/in/terrance-wang/">Terrance Wang</a>,
                <a href="https://www.linkedin.com/in/divi-schmidt-262044180/">Divi Schmidt</a>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
                <br>
                <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="http://www.matthewtancik.com/learnit">project page</a> /
                <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
                <a href="https://arxiv.org/abs/2012.02189">arXiv</a> 
                <p></p>
                <p>Using meta-learning to find weight initializations for coordinate-based MLPs allows them to converge faster and generalize better.</p>
              </td>
            </tr>

            <tr onmouseout="nerfw_stop()" onmouseover="nerfw_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfw_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerfw_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerfw_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerfw_start() {
                    document.getElementById('nerfw_image').style.opacity = "1";
                  }

                  function nerfw_stop() {
                    document.getElementById('nerfw_image').style.opacity = "0";
                  }
                  nerfw_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://nerf-w.github.io/">
                  <span class="papertitle">NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</span>
                </a>
                <br>
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla*</a>,
                <a href="https://scholar.google.com/citations?user=g98QcZUAAAAJ&hl=en">Noha Radwan*</a>,
                <a href="https://research.google/people/105804/">Mehdi S. M. Sajjadi*</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>,
                <a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
                <br>
                <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://nerf-w.github.io/">project page</a> /
                <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">video</a>
                <p></p>
                <p>Letting NeRF reason about occluders and appearance variation produces photorealistic view synthesis using only unstructured internet photos.</p>
              </td>
            </tr> 

            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualrefl_image'>
                    <img src='images/dualrefl_after.jpg' width="160"></div>
                  <img src='images/dualrefl_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function dualrefl_start() {
                    document.getElementById('dualrefl_image').style.opacity = "1";
                  }

                  function dualrefl_stop() {
                    document.getElementById('dualrefl_image').style.opacity = "0";
                  }
                  dualrefl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://sniklaus.com/dualref">
                  <span class="papertitle">Learned Dual-View Reflection Removal</span>
                </a>
                <br>
                <a href="http://sniklaus.com/welcome">Simon Niklaus</a>,
                <a href="https://people.eecs.berkeley.edu/~cecilia77/">Xuaner (Cecilia) Zhang</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="http://web.cecs.pdx.edu/~fliu/">Feng Liu</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>
                <br>
                <em>WACV</em>, 2021
                <br>
                <a href="http://sniklaus.com/dualref">project page</a> /
                <a href="https://arxiv.org/abs/2010.00702">arXiv</a>
                <p></p>
                <p>
                  Reflections and the things behind them often exhibit parallax, and this lets you remove reflections from stereo pairs.
                </p>
              </td>
            </tr> 


            <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nlt_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nlt_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nlt_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nlt_start() {
                    document.getElementById('nlt_image').style.opacity = "1";
                  }

                  function nlt_stop() {
                    document.getElementById('nlt_image').style.opacity = "0";
                  }
                  nlt_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://nlt.csail.mit.edu/">
                  <span class="papertitle">Neural Light Transport for Relighting and View Synthesis</span>
                </a>
                <br>
                <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="http://www.seanfanello.it/">Sean Fanello</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>,
                <a href="https://research.google/people/106687/">Rohit Pandey</a>,
                <a href="https://www.dtic.ua.es/~sorts/">Sergio Orts-Escolano</a>,
                <a href="https://dl.acm.org/profile/99659224296">Philip Davidson</a>,
                <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>,
                <a href="http://www.pauldebevec.com/">Paul Debevec</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <a href="http://billf.mit.edu/">William T. Freeman</a>
                <br>
                <em>ACM TOG</em>, 2021
                <br>
                <a href="http://nlt.csail.mit.edu/">project page</a> /
                <a href="https://arxiv.org/abs/2008.03806">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=OGEnCWZihHE">video</a>
                <p></p>
                <p>Embedding a convnet within a predefined texture atlas enables simultaneous view synthesis and relighting.</p>
              </td>
            </tr> 

            <tr onmouseout="lssr_stop()" onmouseover="lssr_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='lssr_image'>
                    <img src='images/lssr_after.jpg' width="160"></div>
                  <img src='images/lssr_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function lssr_start() {
                    document.getElementById('lssr_image').style.opacity = "1";
                  }

                  function lssr_stop() {
                    document.getElementById('lssr_image').style.opacity = "0";
                  }
                  lssr_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/">
                  <span class="papertitle">Light Stage Super-Resolution: Continuous High-Frequency Relighting</span>
                </a>
                <br>
                <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>
                <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="http://www.seanfanello.it/">Sean Fanello</a>,
                <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>, <br>
                <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2020  
                <br>
                <a href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/">project page</a> / 
                <a href="https://arxiv.org/abs/2010.08888">arXiv</a>
                <p></p>
                <p>
                  Scans for light stages are inherently aliased, but we can use learning to super-resolve them.
                </p>
              </td>
            </tr> 


            <tr onmouseout="ff_stop()" onmouseover="ff_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ff_image'>
                    <img src='images/lion_ff.jpg' width="160"></div>
                  <img src='images/lion_none.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function ff_start() {
                    document.getElementById('ff_image').style.opacity = "1";
                  }

                  function ff_stop() {
                    document.getElementById('ff_image').style.opacity = "0";
                  }
                  ff_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://bmild.github.io/fourfeat/index.html">
                  <span class="papertitle">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</span>
                </a>
                <br>
                <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>, <br>
                <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
                <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
                <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
                <br>
                <em>NeurIPS</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                <br>
                <a href="https://bmild.github.io/fourfeat/">project page</a> /
                video: <a href="https://www.youtube.com/watch?v=nVA6K6Sn2S4">3 min</a>, <a href="https://www.youtube.com/watch?v=iKyIJ_EtSkw">10 min</a> /
                <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
                <a href="https://github.com/tancik/fourier-feature-networks">code</a>
                <p></p>
                <p>Composing neural networks with a simple Fourier feature mapping allows them to learn detailed high-frequency functions.</p>
              </td>
            </tr> 

      
            <tr onmouseout="thresh_stop()" onmouseover="thresh_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='thresh_image'>
                    <img src='images/thresh_after.png' width="160"></div>
                  <img src='images/thresh_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function thresh_start() {
                    document.getElementById('thresh_image').style.opacity = "1";
                  }

                  function thresh_stop() {
                    document.getElementById('thresh_image').style.opacity = "0";
                  }
                  thresh_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2007.07350">
                  <span class="papertitle">A Generalization of Otsu's Method and Minimum Error Thresholding</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>ECCV</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
                <br>
                <a href="https://github.com/jonbarron/hist_thresh">code</a> / 
                <a href="https://www.youtube.com/watch?v=rHtQQlQo1Q4">video</a> / 
                <a href="data/BarronECCV2020.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                A simple and fast Bayesian algorithm that can be written in ~10 lines of code outperforms or matches giant CNNs on image binarization, and unifies three classic thresholding algorithms.
                </p>
              </td>
            </tr>  
      
      
            <tr onmouseout="uflow_stop()" onmouseover="uflow_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='uflow_image'>
                    <img src='images/uflow_after.png' width="160"></div>
                  <img src='images/uflow_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function uflow_start() {
                    document.getElementById('uflow_image').style.opacity = "1";
                  }

                  function uflow_stop() {
                    document.getElementById('uflow_image').style.opacity = "0";
                  }
                  uflow_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2006.04902">
                  <span class="papertitle">What Matters in Unsupervised Optical Flow</span>
                </a>
                <br>
                <a href="http://ricojonschkowski.com/">Rico Jonschkowski</a>,
                <a href="https://www.linkedin.com/in/austin-charles-stone-1ba33b138/">Austin Stone</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="https://research.google/people/ArielGordon/">Ariel Gordon</a>,
                <a href="https://www.linkedin.com/in/kurt-konolige/">Kurt Konolige</a>,
                <a href="https://research.google/people/AneliaAngelova/">Anelia Angelova</a>
                <br>
                <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://github.com/google-research/google-research/tree/master/uflow">code</a>
                <br>
                <p></p>
                <p>
                Extensive experimentation yields a simple optical flow technique that is trained on only unlabeled videos, but still works as well as supervised techniques.
                </p>
              </td>
            </tr>  
      
            <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/vase_small.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/vase_still.png' width="160">
                </div>
                <script type="text/javascript">
                  function nerf_start() {
                    document.getElementById('nerf_image').style.opacity = "1";
                  }

                  function nerf_stop() {
                    document.getElementById('nerf_image').style.opacity = "0";
                  }
                  nerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www.matthewtancik.com/nerf">
                  <span class="papertitle">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</span>
                </a>
                <br>
                <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
                <a href="http://matthewtancik.com/">Matthew Tancik*</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
                <br>
                <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention, CACM Research Highlight)</strong></font>
                <br>
                <a href="http://www.matthewtancik.com/nerf">project page</a>
                /
                <a href="https://arxiv.org/abs/2003.08934">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo&t">talk video</a>
                /
                <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">supp video</a>
                /
                <a href="https://github.com/bmild/nerf">code</a>
                /
                <a href="https://cacm.acm.org/magazines/2022/1/257450-nerf/fulltext">CACM</a> <a href="https://cacm.acm.org/magazines/2022/1/257453-technical-perspective-neural-radiance-fields-explode-on-the-scene/fulltext">(foreward)</a>
                <p></p>
                <p>
                Training a tiny non-convolutional neural network to reproduce a scene using volume rendering achieves photorealistic view synthesis.</p>
              </td>
            </tr> 

            <tr onmouseout="porshadmanip_stop()" onmouseover="porshadmanip_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='porshadmanip_image'>
                    <img src='images/porshadmanip_after.jpg' width="160"></div>
                  <img src='images/porshadmanip_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function porshadmanip_start() {
                    document.getElementById('porshadmanip_image').style.opacity = "1";
                  }

                  function porshadmanip_stop() {
                    document.getElementById('porshadmanip_image').style.opacity = "0";
                  }
                  porshadmanip_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2005.08925">
                  <span class="papertitle">Portrait Shadow Manipulation</span>
                </a>
                <br>
                <a href="https://people.eecs.berkeley.edu/~cecilia77/">Xuaner (Cecilia) Zhang</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>, <br>
                <a href="https://www.linkedin.com/in/rohit-pandey-bab10b7a/">Rohit Pandey</a>,
                <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>
                <br>
                <em>SIGGRAPH</em>, 2020  
                <br>
                <a href="https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait">project page</a> / 
                <a href="https://www.youtube.com/watch?v=M_qYTXhzyac">video</a>
                <p></p>
                <p>Networks can be trained to remove shadows cast on human faces and to soften harsh lighting.</p>
              </td>
            </tr>  

          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
